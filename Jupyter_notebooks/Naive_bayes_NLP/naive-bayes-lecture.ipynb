{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing / Naive Bayes\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Build a Naive Bayes classifier.\n",
    "- Use it to classify documents into categories.\n",
    "\n",
    "# Naive Bayes\n",
    "\n",
    "## Bayes Theorem\n",
    "\n",
    "What is Bayes theorem?\n",
    "\n",
    "$ p(A|B) = \\dfrac{p(B|A) \\cdot p(A)}{p(B)} $\n",
    "\n",
    "\n",
    "## Bayes Theorem Quick Proof\n",
    "\n",
    "Why is this true?\n",
    "\n",
    "Consider this picture.\n",
    "\n",
    "![](images/venn-a-b.png)\n",
    "\n",
    "$ A \\cap B = p(A|B) \\cdot p(B) = p(B|A) \\cdot p(A) $\n",
    "\n",
    "$ p(A|B) \\cdot p(B) = p(B|A) \\cdot p(A) $\n",
    "\n",
    "$ p(A|B) = \\dfrac{p(B|A) \\cdot p(A)}{p(B)} $\n",
    "\n",
    "\n",
    "## Applying Bayes to Classification\n",
    "\n",
    "How can we apply this to classification?\n",
    "\n",
    "$ p(C_k|\\mathbf{x}) = \\dfrac{p(\\mathbf{x}|C_k) \\cdot p(C_k)}{p(\\mathbf{x})} $\n",
    "\n",
    "$ p(C_k|\\mathbf{x}) \\propto p(C_k) \\cdot p(\\mathbf{x}|C_k) $\n",
    "\n",
    "$ p(C_k|\\mathbf{x}) \\propto p(C_k) \\cdot p(x_1,x_2,\\dotsc,x_n|C_k) $\n",
    "\n",
    "$ p(C_k|\\mathbf{x}) \\propto \n",
    "  p(C_k) \\cdot p(x_1|C_k) \\cdot p(x_2|C_k) \\dotsm p(x_n|C_k) $\n",
    "\n",
    "$ p(C_k|\\mathbf{x}) \\propto p(C_k) \\prod_{i=1}^{n} p(x_i|C_k) $\n",
    "\n",
    "## Independence Assumption Step\n",
    "\n",
    "![](images/bayes-classifier-equation.png)\n",
    "\n",
    "## Final Expression\n",
    "\n",
    "$ p(C_k|\\mathbf{x}) \\propto p(C_k) \\prod_{i=1}^{n} p(x_i|C_k) $\n",
    "\n",
    "## Spam\n",
    "\n",
    "<br><details><summary>\n",
    "How can we apply this to spam?\n",
    "</summary>\n",
    "\n",
    "1. We have two categories: spam and not-spam.<br>\n",
    "2. The probability that an email is spam is based on computable probabilities. <br>\n",
    "3. $ p(S|\\mathbf{w}) = \\dfrac{p(\\mathbf{w}|S) \\cdot p(S)}{p(\\mathbf{w})} $<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "How can we get all these probabilities?\n",
    "</summary>\n",
    "\n",
    "1. $p(\\mathbf{w}|S)$ is computed in the training step.<br>\n",
    "2. $p(S)$ is computed in the training step.<br>\n",
    "3. $p(\\mathbf{w})$ can be ignored.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "Why can $p(\\mathbf{w})$ be ignored?\n",
    "</summary>\n",
    "\n",
    "1. Because we only care about maximum likelihood.<br>\n",
    "2. For a specific $\\mathbf{w}$ the value of $p(\\mathbf{w})$ will be the same for all classes (spam and not-spam).<br>\n",
    "</details>\n",
    "\n",
    "## Naive Bayes\n",
    "\n",
    "<br><details><summary>\n",
    "What is Naive Bayes Classification?\n",
    "</summary>\n",
    "\n",
    "1. Naive Bayes classifies text into classes using probabilities.<br>\n",
    "2. It assumes that the probability of each word is independent.<br>\n",
    "3. It works well for text classification.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## When to use it\n",
    "\n",
    "<br><details><summary>\n",
    "Under what conditions does it perform well?\n",
    "</summary>\n",
    "\n",
    "1. n (# of training observations) << p (# of features)<br>\n",
    "2. n somewhat small __or__ p very large<br>\n",
    "3. streams of input data (online learning)<br>\n",
    "4. not bounded by memory; it is possible to keep partially in memory<br>\n",
    "5. Multi-class<br>\n",
    "</details>\n",
    "\n",
    "## Strengths\n",
    "\n",
    "<br><details><summary>\n",
    "What are some strengths of Naive Bayes?\n",
    "</summary>\n",
    "\n",
    "1. Great performance. It is just counting probabilities.<br>\n",
    "2. Online. As new data comes in we can add it to existing probabilities. Don't need to build models from scratch.<br>\n",
    "3. Each class has its own parameterization. So we can add new classes.<br>\n",
    "4. No features are discarded. Even improbable features are incorporated into model. <br>\n",
    "5. Results are interpretable.<br>\n",
    "</details>\n",
    "\n",
    "## Naive Bayes and Text\n",
    "\n",
    "<br><details><summary>\n",
    "Why should Naive Bayes be used with text?\n",
    "</summary>\n",
    "\n",
    "1. Text fits the requirements.<br>\n",
    "2. Using bag-of-words, the number of features is very large (~10,000 - 50,000)<br>\n",
    "3. Usually this is larger than the number of samples.<br>\n",
    "4. So n << p.<br>\n",
    "5. Also empirical evidence shows that it works well.<br>\n",
    "</details>\n",
    "\n",
    "## Feature Independence\n",
    "\n",
    "<br><details><summary>\n",
    "Why is this algorithm called naive?\n",
    "</summary>\n",
    "\n",
    "Because it makes the naive assumption that the features are<br>\n",
    "independent.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "Are the features in text classification independent? For example,\n",
    "\"rolex\" and \"watches\" might occur together. Why is this not hurting\n",
    "us?\n",
    "</summary>\n",
    "\n",
    "1. The words in text classification are frequently not independent.<br>\n",
    "2. All the algorithm cares about is the relative probabilities.<br>\n",
    "3. Dependent features don't affect relative probabilities or the rank of classes.<br>\n",
    "</details>\n",
    "\n",
    "## History \n",
    "\n",
    "<br><details><summary>\n",
    "What is the history of Naive Bayes? \n",
    "</summary>\n",
    "\n",
    "1. Naive Bayes was studied in the 60s.<br>\n",
    "2. It was used in the early 90s for spam filtering.<br>\n",
    "3. In 2002 Paul Graham wrote an essay about using Naive Bayes for spam filtering which popularized it.<br>\n",
    "4. Also Graham tweaked it to reduce the false positive rate.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "How did Paul Graham reduce the false positive rate?\n",
    "</summary>\n",
    "\n",
    "1. Tokenized email headers as well.<br>\n",
    "2. Did not use stemming.<br>\n",
    "3. Decreased false positive rate by double counting words in nonspam dataset.<br>\n",
    "4. Various other techniques. See [essay][pg-spam] for details.<br>\n",
    "</details>\n",
    "\n",
    "[pg-spam]: http://www.paulgraham.com/better.html\n",
    "\n",
    "\n",
    "## Spam Detection\n",
    "\n",
    "<br><details><summary>\n",
    "Consider case sensitivity? For spam detection should you lowercase\n",
    "the tokens?\n",
    "</summary>\n",
    "\n",
    "1. Graham in fact preserved the case.<br>\n",
    "2. This leads to all uppercase emails going straight to the spam folder, which is what you usually want.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What are false positives and negatives in the case of spam? Are they the\n",
    "same?\n",
    "</summary>\n",
    "\n",
    "1. False positives are non-spam emails that get classified as spam.<br>\n",
    "2. For example, your boss's email goes into the spam folder.<br>\n",
    "3. False negatives are spam emails that show up in your inbox.<br>\n",
    "4. False positives are much worse than false negatives.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What features of Naive Bayes helps it as a spam filter?\n",
    "</summary>\n",
    "\n",
    "1. Speed. It is fast. You don't want to delay email.<br>\n",
    "2. Online. It can be run incrementally as training set grows. Does not require rebuilding model from scratch.<br>\n",
    "3. Low false positive. It can be tweaked to have a low false positive rate.<br>\n",
    "</details>\n",
    "\n",
    "## Naive Bayes Intuition\n",
    "\n",
    "What is the Naive Bayes algorithm?\n",
    "\n",
    "- Lets apply the algorithm to classifying articles into topics.\n",
    "- *x* here is a word\n",
    "- *c* is the class (possible topic)\n",
    "- *X* is the document text of a specific document\n",
    "- *x<sub>i</sub>* are all the words in the given document\n",
    "\n",
    "![bayes](images/bayes_rule.png)\n",
    "\n",
    "## The Algorithm\n",
    "\n",
    "![](images/nbc-algorithm.png)\n",
    "\n",
    "## Algorithm Summary\n",
    "\n",
    "<br><details><summary>\n",
    "How can we summarize the algorithm where we apply Naive Bayes?\n",
    "</summary>\n",
    "\n",
    "1. For each document we calculate the probability that the document belongs to a class.<br>\n",
    "2. Then we choose the class with the highest probability.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What do we do in the training phase?\n",
    "</summary>\n",
    "\n",
    "1. We calculate the data structures we need for the classification stage.<br>\n",
    "2. *P(c)*: probability that document belongs to a class (priors).<br>\n",
    "3. *P(x|c)*: probability that the word appears in a class (conditional probabilities).<br>\n",
    "</details>\n",
    "\n",
    "## Priors\n",
    "\n",
    "What are priors?\n",
    "\n",
    "- Priors are the likelihood of each class.\n",
    "- Based on the training set, we can assign a probability to each class.\n",
    "\n",
    "![priors](images/priors.png)\n",
    "\n",
    "- Take a very simple example where 3 classes: sports, politics, arts. \n",
    "- There are 3 sports articles, 4 politics articles and 1 arts articles. \n",
    "- This is 8 articles total. Here are our priors:\n",
    "\n",
    "![priors examples](images/priors_example.png)\n",
    "\n",
    "## Conditional Probability Table\n",
    "\n",
    "How do we compute conditional probability tables?\n",
    "\n",
    "- CPTs contain for every word how many times it appears in each class. \n",
    "- We are calculating the probability that a random word chosen from an\n",
    "  article of class *c* is word *x*.\n",
    "\n",
    "![conditional probability](images/conditional_prob.png)\n",
    "\n",
    "- Again, let's take our example. \n",
    "- Let's look at the word \"ball\". \n",
    "- Here are the occurrences in each of the 8 documents. \n",
    "- We also need the word count of the documents.\n",
    "\n",
    "| Article    | Occurrences of \"ball\" | Total # of words |\n",
    "| :--------- | --------------------: | ---------------: |\n",
    "| Sports 1   |                     5 |              101 |\n",
    "| Sports 2   |                     7 |               93 |\n",
    "| Sports 3   |                     0 |              122 |\n",
    "| Politics 1 |                     0 |               39 |\n",
    "| Politics 2 |                     0 |               81 |\n",
    "| Politics 3 |                     0 |              142 |\n",
    "| Politics 4 |                     0 |               77 |\n",
    "| Arts 1     |                     2 |              198 |\n",
    "\n",
    "- Here are the values in the CPT table for the word \"ball\". \n",
    "- We will do these calculations for all the words that appeared in the\n",
    "  training documents.\n",
    "\n",
    "![cpt example](images/cpt_example.png)\n",
    "\n",
    "## Maximum Likelihood Estimation\n",
    "\n",
    "How do we make a prediction?\n",
    "\n",
    "- To make a prediction we need to pull all this together. \n",
    "- *X* is the content of an article.\n",
    "- *x<sub>1</sub>, x<sub>2</sub>, x<sub>3</sub>, ...* are the words\n",
    "  that make up the article.\n",
    "\n",
    "![mle](images/mle.png)\n",
    "\n",
    "- Assign the topic that has the largest probability. \n",
    "- Note that these \"probabilities\" will not add to 1.\n",
    "\n",
    "## Example\n",
    "\n",
    "How does this apply to a concrete example? \n",
    "\n",
    "- In our example, if we had the very short article of `The Giants beat\n",
    "  the Nationals`, we would do the following calculations:\n",
    "\n",
    "![mle example](images/mle_example.png)\n",
    "\n",
    "- The first probability is the prior.\n",
    "- The remaining come from the Conditional Probability Table.\n",
    "- We make the same calculation for each of the 3 classes and choose\n",
    "  the class with the biggest probability.\n",
    "\n",
    "\n",
    "## Zero Probabilities\n",
    "\n",
    "<br><details><summary>\n",
    "Suppose a word never appears sports article (e.g. \"bayes\"). What will\n",
    "*P(sports|bayes)* be?\n",
    "</summary>\n",
    "\n",
    "It will be zero.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "If there is a sports article *X* with *bayes* in it what will its\n",
    "probability *P(sports|X)* be in our model?\n",
    "</summary>\n",
    "\n",
    "It will be zero.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "How can we fix this?\n",
    "</summary>\n",
    "\n",
    "1. Use Laplace Smoothing.<br>\n",
    "2. Replace these probabilities with 1.<br>\n",
    "</details>\n",
    "\n",
    "## Laplace Smoothing\n",
    "\n",
    "Here is our new probability with Laplace Smoothing.\n",
    "\n",
    "![conditional probability with smoothing](images/conditional_prob_smoothing.png)\n",
    "\n",
    "You can also use a different smoothing constant such as $\\alpha$.\n",
    "\n",
    "![conditional probability with smoothing](images/conditional_prob_smoothing_constant.png)\n",
    "\n",
    "<br><details><summary>\n",
    "What is the advantage of using Laplace Smoothing?\n",
    "</summary>\n",
    "\n",
    "No weird boundary behavior at zero.<br>\n",
    "</details>\n",
    "\n",
    "## Log Likelihood\n",
    "\n",
    "A common convention is to replace the probabilities with log of the\n",
    "probabilities.\n",
    "\n",
    "<br><details><summary>\n",
    "What are some advantages of this?\n",
    "</summary>\n",
    "\n",
    "1. Multiplication becomes addition because $\\log(ab) = \\log(a) + \\log(b)$.<br>\n",
    "2. Word frequencies have a Zipfian distribution.<br>\n",
    "3. This makes their ratio vary linearly instead of exponentially.<br>\n",
    "4. Note that this makes Laplace smoothing required.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What would happen if we used log-probabilities and did not do Laplace smoothing?\n",
    "</summary>\n",
    "\n",
    "$\\log(0) = - \\infty$.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## Log Likelihood MLE\n",
    "\n",
    "Using log-probabilities we actually calculate the *log maximum\n",
    "likelihood error*.\n",
    "\n",
    "![log mle](images/log_mle.png)\n",
    "\n",
    "\n",
    "<br><details><summary>\n",
    "Why does this not affect MLE?\n",
    "</summary>\n",
    "\n",
    "1. Recall that if $a > b$ then $\\log(a) > \\log(b)$.<br>\n",
    "2. So we can still find the maximum of the log likelihoods.<br>\n",
    "3. We only care about rank, not about the values.<br>\n",
    "</details>\n",
    "\n",
    "## Summary of Naive Bayes Algorithm\n",
    "\n",
    "- **Training**: Calculate the priors and Conditional Probability Table\n",
    "- **Predict**: Calculate the MLE for the new article for each label\n",
    "  and pick the max\n",
    "\n",
    "## Various Bayes Models\n",
    "\n",
    "What are the different Naive Bayes models?\n",
    "\n",
    "- Multinomial Bayes\n",
    "- Bernoulli Naive Bayes\n",
    "- Gaussian Naive Bayes\n",
    "\n",
    "## Multinomial Bayes\n",
    "\n",
    "<br><details><summary>\n",
    "What is Multinomial Bayes?\n",
    "</summary>\n",
    "\n",
    "1. The predictors our target variables are multiple classes.<br>\n",
    "2. This is the Bayes algorithm we discussed.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## Bernoulli Naive Bayes\n",
    "\n",
    "<br><details><summary>\n",
    "What is Bernoulli Naive Bayes?\n",
    "</summary>\n",
    "\n",
    "1. Features are independent booleans.<br>\n",
    "2. For example whether a term occurs or not.<br>\n",
    "3. Note that this is not the same as Multinomial Bayes with frequencies truncated to 1.<br>\n",
    "\n",
    "![](images/bernoulli-bayes.png)\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "Why is this not the same as Multinomial Bayes with frequencies\n",
    "truncated to 1?\n",
    "</summary>\n",
    "\n",
    "Because we are explicitly considering the absence of terms.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "Where might Bernoulli Naive Bayes be useful?\n",
    "</summary>\n",
    "\n",
    "Useful for classifying short texts.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## Gaussian Naive Bayes\n",
    "\n",
    "<br><details><summary>\n",
    "What is Gaussian Naive Bayes?\n",
    "</summary>\n",
    "\n",
    "Features are continuous variables.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "<br><details><summary>\n",
    "How can we modify Bayes to handle continuous variables?\n",
    "</summary>\n",
    "\n",
    "One approach is suggested by name Gaussian Naive Bayes:<br>\n",
    "\n",
    "1. Compute the mean and variance of each class.<br>\n",
    "2. Assume the probability distribution is a Gaussian.<br>\n",
    "3. Here is the probability calculation when using the Gaussian approach.<br>\n",
    "\n",
    "![](images/gaussian-bayes.png)<br>\n",
    "\n",
    "Another approach is to convert it to Bernoulli Naive Bayes.\n",
    "\n",
    "1. Use binning to discretize the features.<br>\n",
    "2. Then use Bernoulli Naive Bayes based on whether the a particular feature in a bin is present or not.<br>\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "## Combining\n",
    "\n",
    "<br><details><summary>\n",
    "Suppose you have a mixture of boolean and continuous features. How can\n",
    "you apply Naive Bayes?\n",
    "</summary>\n",
    "\n",
    "This is a limitation of Python's SciKit Learn API. \n",
    "\n",
    "According to Oliver Grisel, one of the main committers of SciKit Learn\n",
    "here are some [options][ogrisel-bayes].<br>\n",
    "\n",
    "1. Use Bernoulli Naive Bayes and use binning for the continuous features.<br>\n",
    "2. Create two classifiers for the two different kinds of features. For details see [here][ogrisel-bayes].<br>\n",
    "3. Write your own Naive Bayes classifier.\n",
    "\n",
    "[ogrisel-bayes]: http://stackoverflow.com/questions/14254203/mixing-categorial-and-continuous-data-in-naive-bayes-classifier-using-scikit-lea\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
