{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing / Naive Bayes\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- Make computers read text.\n",
    "\n",
    "- Turn raw text into features we can feed into our classifiers.\n",
    "\n",
    "- Classify email into ham and spam.\n",
    "\n",
    "- Find out which documents match a search most closely.\n",
    "\n",
    "- Build a Naive Bayes classifier.\n",
    "\n",
    "## Natural Language Processing\n",
    "\n",
    "<br><details><summary>\n",
    "What could computers do if they could read?\n",
    "</summary>\n",
    "\n",
    "1. Filter out email spam.<br>\n",
    "2. Scan resumes.<br>\n",
    "3. Detect plagiarism.<br>\n",
    "4. Classify software bugs into different categories.<br>\n",
    "5. Cluster news like Google News.<br>\n",
    "6. Find out which headlines will get the most clicks.<br>\n",
    "7. Find out which ad copy will get the most clicks.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What is NLP?\n",
    "</summary>\n",
    "\n",
    "1. NLP transforms unstructured text into vectors.<br>\n",
    "2. These vectors can be used for machine learning applications.<br>\n",
    "3. E.g. classification, clustering, regression, recommender systems, etc.<br>\n",
    "4. Instead of just apply ML to numeric data with NLP we can apply it to text.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What is unstructured text? How is it different from structured text?\n",
    "</summary>\n",
    "\n",
    "1. Unstructured text is text without a schema.<br>\n",
    "2. Structured text is text with a schema, e.g. CSV, JSON.<br>\n",
    "3. A schema specifies column names and types for the data.<br>\n",
    "4. Unstructured text has no columns, no specified types.<br>\n",
    "5. Usually is just a blob of text.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What are some sources of unstructured text?\n",
    "</summary>\n",
    "\n",
    "1. Twitter, Facebook, social media.<br>\n",
    "2. Legal documents.<br>\n",
    "3. News stories, blogs, online comments.<br>\n",
    "4. Voice recognition software , OCR, digitized documents.<br>\n",
    "5. Bugs, code comments, documentation.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## Text Classification\n",
    "\n",
    "Suppose we want to classify email into spam and not spam. Consider\n",
    "these two email messages.\n",
    "\n",
    "### Email 1\n",
    "\n",
    "> From: Joe    \n",
    "> Subject: R0lex    \n",
    ">     \n",
    "> Want to buy cheap R0leXX watches?    \n",
    "\n",
    "### Email 2\n",
    "\n",
    "> From: Jim    \n",
    "> Subject: Coffee    \n",
    ">     \n",
    "> Want to grab coffee at 4?    \n",
    "\n",
    "<br><details><summary>\n",
    "Which one of these is likely to be spam?\n",
    "</summary>\n",
    "\n",
    "Email 1.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "<br><details><summary>\n",
    "Why?\n",
    "</summary>\n",
    "\n",
    "1. It contains words that are spammy.<br>\n",
    "2. It contains misspellings.<br>\n",
    "</details>\n",
    "\n",
    "## Vectorizing Text\n",
    "\n",
    "Now as humans we can figure this out pretty easily. Our brains have\n",
    "amazing NLP. But we are not scalable. \n",
    "\n",
    "<br><details><summary>\n",
    "How can we automate this process?\n",
    "</summary>\n",
    "\n",
    "1. Convert text into feature vectors.<br>\n",
    "2. Train classifier on these feature vectors.<br>\n",
    "3. Use output vectors `[1]` to mean spam, and `[0]` to mean not-spam.<br>\n",
    "</details>\n",
    "\n",
    "What is *vectorizing*?\n",
    "\n",
    "- Vectorizing is converting unstructured raw text to feature vectors. \n",
    "\n",
    "Consider these two texts. \n",
    "\n",
    "- Text 1: `Want to buy cheap R0leXX watches?`\n",
    "\n",
    "- Text 2: `Want to grab coffee at 4?`\n",
    "\n",
    "<br><details><summary>\n",
    "How can we vectorize them?\n",
    "</summary>\n",
    "\n",
    "1. Lowercase all words.<br>\n",
    "2. Replace words with common base words.<br>\n",
    "3. Count how many times each word occurs.<br>\n",
    "4. Store as vector.<br>\n",
    "\n",
    "</details>\n",
    "\n",
    "## Vectorized Texts\n",
    "\n",
    "Here is what the emails look like vectorized. This is also known as a\n",
    "*bag of words*.\n",
    "\n",
    "Index |Word     |Text 1   |Text 2\n",
    "----- |----     |------   |------\n",
    "0     |want     |1        |1\n",
    "1     |to       |1        |1\n",
    "2     |buy      |1        |0\n",
    "3     |cheap    |1        |0\n",
    "4     |r0lexx   |1        |0\n",
    "5     |watches  |1        |0\n",
    "6     |grab     |0        |1\n",
    "7     |coffee   |0        |1\n",
    "8     |at       |0        |1\n",
    "9     |4        |0        |1\n",
    "\n",
    "## Terms\n",
    "\n",
    "Term         |Meaning\n",
    "----         |-------\n",
    "Corpus       |Collection of documents (collection of articles).\n",
    "Document     |A single document (a tweet, an email, an article).\n",
    "Vocabulary   |Set of words in your corpus, or maybe the entire English dictionary. \n",
    "Bag of Words |Vector representation of words in a document.\n",
    "Token        |Single word.\n",
    "Stop Words   |Common ignored words because not useful in distinguishing text.\n",
    "Vectorizing  |Converting text into a bag-of-words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Universal Parts of Speech\n",
    "\n",
    "<table border=\"1\" class=\"docutils\" id=\"tab-universal-tagset\">\n",
    "<colgroup>\n",
    "<col width=\"11%\">\n",
    "<col width=\"27%\">\n",
    "<col width=\"62%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr><th class=\"head\">Tag</th>\n",
    "<th class=\"head\">Meaning</th>\n",
    "<th class=\"head\">English Examples</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">ADJ</span></tt></td>\n",
    "<td>adjective</td>\n",
    "<td><span class=\"example\">new, good, high, special, big, local</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">ADP</span></tt></td>\n",
    "<td>adposition</td>\n",
    "<td><span class=\"example\">on, of, at, with, by, into, under</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">ADV</span></tt></td>\n",
    "<td>adverb</td>\n",
    "<td><span class=\"example\">really, already, still, early, now</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">CONJ</span></tt></td>\n",
    "<td>conjunction</td>\n",
    "<td><span class=\"example\">and, or, but, if, while, although</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">DET</span></tt></td>\n",
    "<td>determiner, article</td>\n",
    "<td><span class=\"example\">the, a, some, most, every, no, which</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">NOUN</span></tt></td>\n",
    "<td>noun</td>\n",
    "<td><span class=\"example\">year, home, costs, time, Africa</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">NUM</span></tt></td>\n",
    "<td>numeral</td>\n",
    "<td><span class=\"example\">twenty-four, fourth, 1991, 14:24</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">PRT</span></tt></td>\n",
    "<td>particle</td>\n",
    "<td><span class=\"example\">at, on, out, over per, that, up, with</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">PRON</span></tt></td>\n",
    "<td>pronoun</td>\n",
    "<td><span class=\"example\">he, their, her, its, my, I, us</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">VERB</span></tt></td>\n",
    "<td>verb</td>\n",
    "<td><span class=\"example\">is, say, told, given, playing, would</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">.</span></tt></td>\n",
    "<td>punctuation marks</td>\n",
    "<td><span class=\"example\">. , ; !</span></td>\n",
    "</tr>\n",
    "<tr><td><tt class=\"doctest\"><span class=\"pre\">X</span></tt></td>\n",
    "<td>other</td>\n",
    "<td><span class=\"example\">ersatz, esprit, dunno, gr8, univeristy</span></td>\n",
    "</tr>\n",
    "</tbody>\n",
    "\n",
    "\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building an NLP Pipeline\n",
    "\n",
    "Lets build an NLP Pipeline to turn unstructured text data into\n",
    "something we can train a classifier on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import nltk\n",
    "# nltk.download('all');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing\n",
    "\n",
    "The first step is turning your raw text documents into lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['want', 'to', 'buy', 'cheap', 'r0lexx', 'watches', '?'], ['want', 'to', 'grab', 'coffee', 'at', '4', '?'], ['what', 'time', 'works', 'for', 'you', ',', 'for', 'coffee', 'after', 'work', '?']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "document1 = 'Want to buy cheap R0leXX watches?'\n",
    "document2 = 'Want to grab coffee at 4?'\n",
    "document3 = 'What time works for you, for coffee after work?'\n",
    "documents = [document1, document2, document3]\n",
    "corpus = [word_tokenize(content.lower()) for content in documents]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['want', 'to', 'buy', 'cheap', 'r0lexx', 'watches', '?'], ['want', 'to', 'grab', 'coffee', 'at', '4', '?'], ['what', 'time', 'works', 'for', 'you', ',', 'for', 'coffee', 'after', 'work', '?']]\n"
     ]
    }
   ],
   "source": [
    "# or you can do it like this (or any number of ways!)\n",
    "documents = \" \".join([document1, document2, document3])\n",
    "print([word_tokenize(content.lower()) for content in sent_tokenize(documents)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question\n",
    "\n",
    "<br><details><summary>\n",
    "What are some use cases for sentence tokenizing?\n",
    "</summary>\n",
    "\n",
    "1. Classifying repetitive text that uses the same sentences.<br>\n",
    "2. Classifying conversation between an airplane and a control tower.<br>\n",
    "3. Classifying text generated by filling out a form.<br>\n",
    "4. Breaking document down into sentences and treating each sentence as\n",
    "   a document. For example, to give MPAA rating to a movie script.<br>\n",
    "</details>\n",
    "\n",
    "## Stop Words\n",
    "\n",
    "NLTK has functionality for removing common words that show up in\n",
    "almost every document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', 'couldn', 'didn', 'doesn', 'hadn', 'hasn', 'haven', 'isn', 'ma', 'mightn', 'mustn', 'needn', 'shan', 'shouldn', 'wasn', 'weren', 'won', 'wouldn']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['want', 'buy', 'cheap', 'r0lexx', 'watches', '?'],\n",
      " ['want', 'grab', 'coffee', '4', '?'],\n",
      " ['time', 'works', ',', 'coffee', 'work', '?']]\n"
     ]
    }
   ],
   "source": [
    "# Remove stop words\n",
    "corpus = [[token for token in document if token not in stop_words] for document in corpus]\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punctuation\n",
    "In general, we can drop punctuation. It does not jive with the tf-idf technique we're about to learn..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['want', 'buy', 'cheap', 'r0lexx', 'watches'],\n",
      " ['want', 'grab', 'coffee', '4'],\n",
      " ['time', 'works', 'coffee', 'work']]\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuation = set(string.punctuation)\n",
    "corpus = [[token for token in document if token not in punctuation] for document in corpus]\n",
    "pprint(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "\n",
    "<br><details><summary>\n",
    "What might be some applications where you don't want to remove stop\n",
    "words from your text?\n",
    "</summary>\n",
    "\n",
    "1. Plagiarism detection.<br>\n",
    "2. Investigating if dusty papers found in attic are a lost Shakespeare play.<br>\n",
    "3. Identifying document types in Data Loss Prevention. E.g. resume, \n",
    "   insider information, legal contract, etc.<br> \n",
    "</details>\n",
    "\n",
    "## Stemming and Lemmatization\n",
    "\n",
    "<br><details><summary>\n",
    "How will our code treat words like \"watch\" and \"watches\"?\n",
    "</summary>\n",
    "\n",
    "It will treat them as different words.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "How can we fix this?\n",
    "</summary>\n",
    "\n",
    "1. Remove inflectional endings and return base form of word (known as the lemma).<br>\n",
    "2. This is known as stemming or lemmatization.<br>\n",
    "</details>\n",
    "\n",
    "What is the difference between stemming and lemmatization?\n",
    "\n",
    "- Lemmatization is more general.\n",
    "\n",
    "- Converting *cars* to *car* is stemming.\n",
    "\n",
    "- Converting *automobile* to *car* is lemmatization.\n",
    "\n",
    "- Behavior depends on your toolkit.\n",
    "\n",
    "In Python's NLTK:\n",
    "\n",
    "- Stemming converts *cars* to *car*, but does not convert *children*\n",
    "  to *child*.\n",
    "\n",
    "- Lemmatization converts both.\n",
    "\n",
    "## Stemming and Lemmatization Computation\n",
    "\n",
    "Removing morphoglical affixes from words, and also replacing words with their\n",
    "\"lemma\" (or base words: \"good\" is the lemma of \"better\").\n",
    "\n",
    "- running -> run\n",
    "\n",
    "- generously -> generous\n",
    "\n",
    "- better -> good\n",
    "\n",
    "- dogs -> dog\n",
    "\n",
    "- mice -> mouse\n",
    "\n",
    "Here is how to do this using NLTK. Don't try to write your own\n",
    "functions for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter   import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet  import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer('english')\n",
    "wordnet = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "mouse\n",
      "goose\n"
     ]
    }
   ],
   "source": [
    "print(snowball.stem('running'))\n",
    "print(wordnet.lemmatize('mice'))\n",
    "print(wordnet.lemmatize('geese'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['want', 'buy', 'cheap', 'r0lexx', 'watch'],\n",
       " ['want', 'grab', 'coffee', '4'],\n",
       " ['time', 'work', 'coffee', 'work']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = [[wordnet.lemmatize(word) for word in document] for document in corpus]\n",
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "Build you vocabulary from the set of tokens contained in your corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cheap',\n",
       " 'r0lexx',\n",
       " 'grab',\n",
       " 'time',\n",
       " 'buy',\n",
       " 'watch',\n",
       " 'coffee',\n",
       " '4',\n",
       " 'want',\n",
       " 'work']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_set = set()\n",
    "[[vocab_set.add(token) for token in tokens] for tokens in corpus]\n",
    "vocab = list(vocab_set)\n",
    "vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-grams\n",
    "\n",
    "<br><details><summary>\n",
    "Consider \"rolex watch\" vs \"lets watch the game this weekend\". Which\n",
    "one is spam?\n",
    "</summary>\n",
    "\n",
    "The first one.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "Why use N-grams?\n",
    "</summary>\n",
    "\n",
    "1. Sometimes context makes a difference.<br>\n",
    "2. You want to see the words before and after.<br>\n",
    "3. N-grams are strings consecutive words in your corpus.<br>\n",
    "4. These are extra \"features\" in your data that contain more information that individual words.<br>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n-grams:\n",
      "-----------\n",
      "[[('want', 'buy'), ('buy', 'cheap'), ('cheap', 'r0lexx'), ('r0lexx', 'watch')],\n",
      " [('want', 'grab'), ('grab', 'coffee'), ('coffee', '4')],\n",
      " [('time', 'work'), ('work', 'coffee'), ('coffee', 'work')]]\n",
      "skip grams:\n",
      "-----------\n",
      "[[('want', 'buy'),\n",
      "  ('want', 'cheap'),\n",
      "  ('buy', 'cheap'),\n",
      "  ('buy', 'r0lexx'),\n",
      "  ('cheap', 'r0lexx'),\n",
      "  ('cheap', 'watch'),\n",
      "  ('r0lexx', 'watch')],\n",
      " [('want', 'grab'),\n",
      "  ('want', 'coffee'),\n",
      "  ('grab', 'coffee'),\n",
      "  ('grab', '4'),\n",
      "  ('coffee', '4')],\n",
      " [('time', 'work'),\n",
      "  ('time', 'coffee'),\n",
      "  ('work', 'coffee'),\n",
      "  ('work', 'work'),\n",
      "  ('coffee', 'work')]]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams, skipgrams\n",
    "from pprint import pprint\n",
    "\n",
    "# An n-gram is a sequence of n words\n",
    "# when n = 2, we call this a bigram. when n=3, trigram, etc...\n",
    "bigrams = [list(ngrams(sequence=document, n=2)) for document in corpus]\n",
    "\n",
    "# Skipgrams are ngrams that allows tokens to be skipped.\n",
    "skipgrams = [list(skipgrams(sequence=document, n=2, k=1)) for document in corpus]\n",
    "\n",
    "print('n-grams:')\n",
    "print('-----------')\n",
    "pprint(bigrams)\n",
    "\n",
    "print('skip grams:')\n",
    "print('-----------')\n",
    "pprint(skipgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words\n",
    "\n",
    "How do you turn a document into a bag-of-words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def vectorize(doc, vocabulary):\n",
    "    bag_of_words = Counter(doc.split(' '))\n",
    "    doc_vector = np.zeros(len(vocabulary))\n",
    "    for word_index, word in enumerate(vocabulary):\n",
    "        if word in bag_of_words:\n",
    "            doc_vector[word_index] += bag_of_words[word]\n",
    "    return doc_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = [' '.join(tokens) for tokens in corpus]\n",
    "vectorized_documents = [vectorize(document, vocab) for document in corpus]\n",
    "term_frequency_matrix = np.vstack(vectorized_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' cheap', 'r0lexx', '  grab', '  time', '   buy', ' watch', 'coffee', '     4', '  want', '  work']\n",
      "['     1', '     1', '     0', '     0', '     1', '     1', '     0', '     0', '     1', '     0']\n",
      "['     0', '     0', '     1', '     0', '     0', '     0', '     1', '     1', '     1', '     0']\n",
      "['     0', '     0', '     0', '     1', '     0', '     0', '     1', '     0', '     0', '     2']\n"
     ]
    }
   ],
   "source": [
    "print([\"{:>6}\".format(token) for token in vocab])\n",
    "for row in term_frequency_matrix:\n",
    "    print([\"{:>6}\".format(str(int(value))) for value in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'want': 6, 'buy': 0, 'cheap': 1, 'r0lexx': 4, 'watch': 7, 'grab': 3, 'coffee': 2, 'time': 5, 'work': 8}\n",
      "['buy', 'cheap', 'coffee', 'grab', 'r0lexx', 'time', 'want', 'watch', 'work']\n"
     ]
    }
   ],
   "source": [
    "# lucky for us, Sklearn has built-in methods to do this efficiently.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "c = CountVectorizer(stop_words='english')\n",
    "bag_of_words = c.fit(corpus)\n",
    "\n",
    "feature_dict = bag_of_words.vocabulary_ # with numerical indices\n",
    "print(feature_dict)\n",
    "\n",
    "feature_list = bag_of_words.get_feature_names() # just the names, alphabetically\n",
    "print(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['   buy', ' cheap', 'coffee', '  grab', 'r0lexx', '  time', '  want', ' watch', '  work']\n",
      "['     1', '     1', '     0', '     0', '     1', '     0', '     1', '     1', '     0']\n",
      "['     0', '     0', '     1', '     1', '     0', '     0', '     1', '     0', '     0']\n",
      "['     0', '     0', '     1', '     0', '     0', '     1', '     0', '     0', '     2']\n"
     ]
    }
   ],
   "source": [
    "# finally we can convert this to a vector such that we can (soon) apply machine learning!\n",
    "term_frequency_matrix = c.fit_transform(corpus).toarray()\n",
    "\n",
    "print([\"{:>6}\".format(token) for token in feature_list])\n",
    "for row in term_frequency_matrix:\n",
    "    print([\"{:>6}\".format(str(value)) for value in row])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Limitations\n",
    "\n",
    "<br><details><summary>\n",
    "What are some limitations of the bag-of-words approach and\n",
    "CountVectorizer?\n",
    "</summary>\n",
    "\n",
    "1. Longer documents weigh more than short documents.<br>\n",
    "2. Does not consider uniqueness of words. Unique words should weigh more.<br>\n",
    "3. We are losing a lot of structure. This is like a giant word grinder.<br>\n",
    "4. We will address the first two issues. The third issue is part of the bargain we struck with the bag-of-words approach.<br>\n",
    "5. Note: bag-of-words is not the only way to featurize text. It is simple, and surprisingly powerful.<br>\n",
    "</details>\n",
    "\n",
    "## L2 Normalization\n",
    "\n",
    "<br><details><summary>\n",
    "What is L2 normalization?\n",
    "</summary>\n",
    "\n",
    "1. Divide each vector by its L2-norm.<br>\n",
    "2. Divide each vector by its magnitude.<br>\n",
    "3. Divide each vector by square root of sum of squares of all elements.<br>\n",
    "4. Makes long and short documents weigh the same.<br>\n",
    "$$\\frac{\\vec{v}}{||v_i||}$$<br>\n",
    "$$\\frac{\\vec{v}}{\\sqrt[2]{\\sum{v_i^2}}}$$\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What is L1 normalization? \n",
    "</summary>\n",
    "\n",
    "Divide each vector by its L1-norm.<br>\n",
    "\n",
    "$$\\frac{\\vec{v}}{\\sum{|v_i|}}$$<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What is L(n) normalization?\n",
    "</summary>\n",
    "\n",
    "Divide each vector by its L(n)-norm.<br>\n",
    "\n",
    "$$\\frac{\\vec{v}}{\\sqrt[n]{\\sum{|v_i|^n}}}$$<br>\n",
    "</details>\n",
    "\n",
    "## Why L2?\n",
    "\n",
    "<br><details><summary>\n",
    "Why use the L2-norm?\n",
    "</summary>\n",
    "\n",
    "1. It makes dot products between vectors meaningful.<br>\n",
    "2. Will see this later with cosine similarity.<br>\n",
    "</details>\n",
    "\n",
    "## Dot Product\n",
    "\n",
    "<br><details><summary>\n",
    "If two documents are highly similar what will the dot product of their\n",
    "L2-normalized vectors be?\n",
    "</summary>\n",
    "\n",
    "The dot product will be 1.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What does a dot product of 0 indicate?\n",
    "</summary>\n",
    "\n",
    "There is no similarity between the documents.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What does a dot product of -1 indicate?\n",
    "</summary>\n",
    "\n",
    "This is not possible since all vector values are zero or positive.<br>\n",
    "</details>\n",
    "\n",
    "## TF-IDF Intuition\n",
    "\n",
    "Intuitively, the idea of TF-IDF is this:\n",
    "\n",
    "- Words that occur in every document are less useful than words that\n",
    "  only occur in some documents.\n",
    "\n",
    "- Instead of looking at the term frequency for each word in a document\n",
    "  we want to scale up terms that are rare.\n",
    "\n",
    "## Applications\n",
    "\n",
    "<br><details><summary>\n",
    "Which term is likely to be more significant in spam detection:\n",
    "\"hey\", \"rolex\", and why?\n",
    "</summary>\n",
    "\n",
    "1. \"Rolex\" is going to be more significant.<br>\n",
    "2. Because this is a unique word that does not occur in a lot of documents.<br>\n",
    "3. \"Hey\" is a lot more common and is less likely to be a useful feature.<br>\n",
    "</details>\n",
    "\n",
    "## Inverse Document Frequency\n",
    "\n",
    "How can we increase the weight of tokens that are rare in our corpus\n",
    "and decrease the weight of tokens that are common?\n",
    "\n",
    "- Use Inverse Document Frequency.\n",
    "\n",
    "- Inverse Document Frequency or IDF is a measure of how unique a term\n",
    "  is. \n",
    "  \n",
    "- So we want to weigh terms high if they are unique.\n",
    "\n",
    "What is the formula for IDF?\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- *t* is a token\n",
    "\n",
    "- *d* is a document\n",
    "\n",
    "- *D* is a corpus\n",
    "\n",
    "- *N* is the total number of documents in *D*\n",
    "\n",
    "- *n(t)* is the number of documents containing *t*\n",
    "\n",
    "$$idf(t, d) = \\log{\\left(\\frac{N}{n}\\right)}$$\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "What is TF-IDF?\n",
    "\n",
    "- TF-IDF combines TF or the normalized token counts.\n",
    "- Then it multiplies it with IDF.\n",
    "\n",
    "What is the formula for TF-IDF?\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- *t* is a token\n",
    "\n",
    "- *d* is a document\n",
    "\n",
    "- *D* is a corpus\n",
    "\n",
    "- *N* is the total number of documents in *D*\n",
    "\n",
    "- *n(t)* is the number of documents containing *t*\n",
    "\n",
    "Then, TF-IDF is:\n",
    "\n",
    "$$tfidf(t,d) = tf(t,d) * idf(t,d)$$\n",
    "\n",
    "What is TF?\n",
    "\n",
    "- TF is the number of times that the token $t$ appears in $d$ (often\n",
    "  normalized by dividing by the total length of $d$).\n",
    "\n",
    "$$tf(t, d) = freq(t, d)$$\n",
    "\n",
    "What is IDF?\n",
    "\n",
    "- IDF is a score for how unique a token is across the corpus.\n",
    "\n",
    "$$idf(t, d) = \\log{\\left(\\frac{N}{n}\\right)}$$\n",
    "\n",
    "- This is sometimes written with some smoothers like this:\n",
    "\n",
    "$$idf(t, d) = \\log{\\left(\\frac{N + 1}{n + 1}\\right)} + 1$$\n",
    "\n",
    "## Adding Ones\n",
    "\n",
    "<br><details><summary>\n",
    "Why are we adding 1's?\n",
    "</summary>\n",
    "\n",
    "1. This is called smoothing.\n",
    "2. Adding 1 inside the log ensures that we never divide by 0.<br>\n",
    "3. Adding 1 at the end ensures that *idf* is always non-zero.<br>\n",
    "</details>\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "Consider a very small library with these books:\n",
    "\n",
    "- Hadoop Handbook (HH)\n",
    "- Beekeeping Bible (BB)\n",
    "\n",
    "Here are the word frequencies.\n",
    "\n",
    "Terms  |HH   |BB\n",
    "-----  |--   |--\n",
    "hadoop |100  |0\n",
    "bees   |0    |150\n",
    "hive   |20   |50\n",
    "\n",
    "<br><details><summary>\n",
    "Intuitively what do you expect the IDF scores of hadoop, bees, and\n",
    "hive to be?\n",
    "</summary>\n",
    "\n",
    "1. Hadoop and bees should have a higher score.<br>\n",
    "2. Hive should have a low score because it is not rare.<br>\n",
    "</details>\n",
    "\n",
    "<br><details><summary>\n",
    "What are the IDF scores of hadoop, bees, and hive? Assume log base 2.\n",
    "</summary>\n",
    "\n",
    "1. Note these are independent of document.<br>\n",
    "2. For hadoop: $N = 2, n = 1, \\log(N/n) = \\log(2) = 1$.<br>\n",
    "3. For bees: $N = 2, n = 1, \\log(N/n) = \\log(2) = 1$.<br>\n",
    "4. For hive: $N = 2, n = 2, \\log(N/n) = \\log(1) = 0$.<br>\n",
    "</details>\n",
    "\n",
    "## Computing  TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Frequency: [1 1 2 1 1 1 2 1 1]\n",
      "Number of Documents: 3\n",
      "L2 Normalization: \n",
      "[[ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n",
      "Inverse Document Frequeny: \n",
      "[ 1.69314718  1.69314718  1.28768207  1.69314718  1.69314718  1.69314718\n",
      "  1.28768207  1.69314718  1.69314718]\n",
      "Term Frequency-Inverse Document Frequence: \n",
      "[[ 0.75719844  0.75719844  0.          0.          0.75719844  0.\n",
      "   0.57586893  0.75719844  0.        ]\n",
      " [ 0.          0.          0.74344359  0.97753898  0.          0.\n",
      "   0.74344359  0.          0.        ]\n",
      " [ 0.          0.          0.525694    0.          0.          0.69122444\n",
      "   0.          0.          1.38244888]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "\n",
    "# compute total number of times each token appears across all documents\n",
    "document_freq = np.sum(term_frequency_matrix > 0, axis=0)\n",
    "print('Document Frequency: {}'.format(document_freq))\n",
    "\n",
    "# N is the number of documents\n",
    "N = term_frequency_matrix.shape[0]\n",
    "print('Number of Documents: {}'.format(N))\n",
    "\n",
    "# Divide each row by its L2 norm\n",
    "L2_rows = np.sqrt(np.sum(term_frequency_matrix**2, axis=1)).reshape(N, 1)\n",
    "term_frequency_matrix = term_frequency_matrix / L2_rows\n",
    "print('L2 Normalization: \\n{}'.format(L2_rows))\n",
    "\n",
    "# Add a smoother to keep IDF values for words that do not appear in a given document\n",
    "idf = np.log(float(N+1) / (1.0 + document_freq)) + 1.0\n",
    "print('Inverse Document Frequeny: \\n{}'.format(idf))\n",
    "\n",
    "tfidf = np.multiply(term_frequency_matrix, idf)\n",
    "\n",
    "# tfidf = normalize(tfidf, norm='l2', axis=1)\n",
    "print('Term Frequency-Inverse Document Frequence: \\n{}'.format(tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.75719844,  0.75719844,  0.        ,  0.        ,  0.75719844,\n",
       "         0.        ,  0.57586893,  0.75719844,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.74344359,  0.97753898,  0.        ,\n",
       "         0.        ,  0.74344359,  0.        ,  0.        ],\n",
       "       [ 0.        ,  0.        ,  0.525694  ,  0.        ,  0.        ,\n",
       "         0.69122444,  0.        ,  0.        ,  1.38244888]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_frequency_matrix*idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF with Sklearn\n",
    "Instead of all that code above, you can just do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.46735098  0.46735098  0.          0.          0.46735098  0.\n",
      "   0.35543247  0.46735098  0.        ]\n",
      " [ 0.          0.          0.51785612  0.68091856  0.          0.\n",
      "   0.51785612  0.          0.        ]\n",
      " [ 0.          0.          0.32200242  0.          0.          0.42339448\n",
      "   0.          0.          0.84678897]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "z = TfidfTransformer(norm='l2')\n",
    "print(z.fit_transform(term_frequency_matrix).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting It All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using newsgroups data set, let's fetch all of it.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "\n",
    "# Subset the data, this dataset is huge.\n",
    "cats = ['alt.atheism', 'sci.space']\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', categories=cats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'From: schaefer@sal-sun121.usc.edu (Peter Schaefer)\\nSubject: Re: Why not give $1 billion to first year-long moon residents?\\nOrganization: University of Southern California, Los Angeles, CA\\nLines: 29\\nDistribution: world\\nNNTP-Posting-Host: sal-sun121.usc.edu\\n\\n\\nIn article <1993Apr19.130503.1@aurora.alaska.edu>, nsmca@aurora.alaska.edu writes:\\n|> In article <6ZV82B2w165w@theporch.raider.net>, gene@theporch.raider.net (Gene Wright) writes:\\n|> > With the continuin talk about the \"End of the Space Age\" and complaints \\n|> > by government over the large cost, why not try something I read about \\n|> > that might just work.\\n|> > \\n|> > Announce that a reward of $1 billion would go to the first corporation \\n|> > who successfully keeps at least 1 person alive on the moon for a year. \\n|> > Then you\\'d see some of the inexpensive but not popular technologies begin \\n|> > to be developed. THere\\'d be a different kind of space race then!\\n|> > \\n|> > --\\n|> >   gene@theporch.raider.net (Gene Wright)\\n|> > theporch.raider.net  615/297-7951 The MacInteresteds of Nashville\\n|> ====\\n|> If that were true, I\\'d go for it.. I have a few friends who we could pool our\\n|> resources and do it.. Maybe make it a prize kind of liek the \"Solar Car Race\"\\n|> in Australia..\\n|> Anybody game for a contest!\\n|> \\n|> ==\\n|> Michael Adams, nsmca@acad3.alaska.edu -- I\\'m not high, just jacked\\n\\n\\nOh gee, a billion dollars!  That\\'d be just about enough to cover the cost of the\\nfeasability study!  Happy, Happy, JOY! JOY!\\n\\nPeter F. Schaefer\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take a peek at the data\n",
    "random_index = np.random.randint(len(newsgroups_train['data']))\n",
    "newsgroups_train['data'][random_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.         0.         0.        ...,  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...,  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...,  0.         0.         0.       ]\n",
      " ..., \n",
      " [ 0.         0.         0.        ...,  0.         0.         0.       ]\n",
      " [ 0.         0.0476151  0.        ...,  0.         0.         0.       ]\n",
      " [ 0.         0.         0.        ...,  0.         0.         0.       ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(stop_words=stopwords.words('english'), norm='l2')\n",
    "print(tfidf.fit_transform(newsgroups_train['data']).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine Similarity\n",
    "\n",
    "We need a way to compare our documents. Use the cosine similary metric!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "doc0 = 'the cat in the hat'\n",
    "doc1 = 'the cat ate my hat'\n",
    "doc2 = 'the cat in the hat the cat in the hat the cat in the hat'\n",
    "doc3 = 'kale is on sale'\n",
    "corpus = [doc0, doc1, doc2, doc3]\n",
    "\n",
    "vocab = sorted(list(set([word for doc in corpus for word in doc.split(' ')])))\n",
    "term_frequency_matrix = np.vstack([vectorize(doc, vocab) for doc in corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['   ate', '   cat', '   hat', '    in', '    is', '  kale', '    my', '    on', '  sale', '   the']\n",
      "['     0', '     1', '     1', '     1', '     0', '     0', '     0', '     0', '     0', '     2']\n",
      "['     1', '     1', '     1', '     0', '     0', '     0', '     1', '     0', '     0', '     1']\n",
      "['     0', '     3', '     3', '     3', '     0', '     0', '     0', '     0', '     0', '     6']\n",
      "['     0', '     0', '     0', '     0', '     1', '     1', '     0', '     1', '     1', '     0']\n"
     ]
    }
   ],
   "source": [
    "print([\"{:>6}\".format(token) for token in vocab])\n",
    "for row in term_frequency_matrix:\n",
    "    print([\"{:>6}\".format(str(int(value))) for value in row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(0, 1)': '0.6761',\n",
       " '(0, 2)': '1.0000',\n",
       " '(0, 3)': '0.0000',\n",
       " '(1, 2)': '0.6761',\n",
       " '(1, 3)': '0.0000',\n",
       " '(2, 3)': '0.0000'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "indices = list(range(term_frequency_matrix.shape[0]))\n",
    "combinations = list(itertools.combinations(indices, 2))\n",
    "similarity_dict = {}\n",
    "for pair in combinations:\n",
    "    similarity = cosine_similarity(\n",
    "        term_frequency_matrix[pair[0]].reshape(1,-1), \n",
    "        term_frequency_matrix[pair[1]].reshape(1,-1)\n",
    "    )\n",
    "    similarity_dict.update({'{}'.format(pair) : '{:.4f}'.format(similarity.squeeze().round(4))})\n",
    "\n",
    "similarity_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Trick\n",
    "\n",
    "One of the limitations of CountVectorizer is that the vectors it\n",
    "produces can be very large. \n",
    "\n",
    "<br><details><summary>\n",
    "How can we fix this?\n",
    "</summary>\n",
    "\n",
    "1. Use `HashingVectorizer`.<br>\n",
    "2. Hash words to collapse the vector.<br>\n",
    "3. Vector still retains enough uniqueness to be useful.<br>\n",
    "</details>\n",
    "\n",
    "## Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.          0.          0.33333333  0.          0.\n",
      "   0.         -0.66666667  0.66666667]\n",
      " [ 0.4472136   0.4472136   0.          0.          0.4472136   0.          0.\n",
      "   0.         -0.4472136   0.4472136 ]\n",
      " [ 0.          0.          0.          0.          0.33333333  0.          0.\n",
      "   0.         -0.66666667  0.66666667]\n",
      " [ 0.         -0.5         0.          0.          0.          0.5         0.\n",
      "   0.5        -0.5         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "hv = HashingVectorizer(n_features=10)\n",
    "features = hv.transform(corpus)\n",
    "print(features.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize\n",
    "\n",
    "<br><details><summary>\n",
    "What are some steps in vectorizing text?\n",
    "</summary>\n",
    "\n",
    "1. Tokenize.<br>\n",
    "2. Stemming, lemmatization, lowercasing, etc.<br>\n",
    "3. Count frequencies.<br>\n",
    "4. Modify feature weights using TF-IDF<br>\n",
    "5. Divide by L2 norm.<br>\n",
    "6. Use hashing trick.<br>\n",
    "</details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
