{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "Linear Algebra: Instantiate matrices A and B, and vector x like:\n",
    "  ``` python\n",
    "  A = np.random.randint(0, 50, size=(4, 4))\n",
    "  B = np.random.randint(0, 50, size=(4, 3))\n",
    "  x = np.random.randint(0, 50, size=(20, ))\n",
    "  ```\n",
    "  Note that the answers will all be different because you have used random numbers to create the matrices. The important piece here is the code to perform the operation. These should all be one line of code\n",
    "### Answer\n",
    "\n",
    "  * Return only the elements of x that are even.\n",
    "      [num for num in x if num % 2 = 0]\n",
    "      x[x%2 ==0]\n",
    "      np.extract(np.mod=(x, 2)=0)x)\n",
    "  * Multiply matrices A and B.\n",
    "       A.dot(B)\n",
    "  * Compute the inverse of A. Can B be inverted? Why or why not?\n",
    "       linalg.inv(A)\n",
    "  * Compute the euclidean distance between rows 1 and 3 of matrix A.\n",
    "       np.linalg.norm(A[0,:]-A[2,:])\n",
    "  * Compute the cosine similarity between columns 1 and 2 of matrix A.\n",
    "       np.dot(A[:,0], A[:,1]/np.linalg.norm(A[:,0] + np.linalg.norm(A[:,0])\n",
    "  * Find the matrix C such that B * C = A (write C in terms of A and B)\n",
    "       (XT*X)-1*XT*y = B\n",
    "       (BT*B)-1*BT*A=C\n",
    "  * Find the eigenvalues and eigenvectors of matrix A. Describe what eigenvalues and eigenvectors tell you.\n",
    "      np.linalg.eig(A), if you were print this you'd get one array, 4x1. with four eigan values,(lambdas). second array is a 4x4, the first column is the eigan vector associated with lambda 1, second with lambda 2, etc\n",
    "      \n",
    "      Ax = lambdax\n",
    "      A is the eigan vetor, describes the direction\n",
    "      value is the lambda, describes the magnitude\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "What are the assumptions behind OLS linear regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer 3\n",
    "Linearity\n",
    "\n",
    "    asumming a linear relationship between beta and x \n",
    "    if its not x, it might be 1/x or log(x) or x^2\n",
    "    feature engineering to transform relationships\n",
    "    inspect data in pairwise scatterplots\n",
    "\n",
    "Independence\n",
    "\n",
    "    X represented the independent variables, not y\n",
    "    assuming the exogeneity of regressors\n",
    "    confirmed by visual inspections\n",
    "    expected value of the residuals given x is zero\n",
    "    No linear dependence among features of X\n",
    "\n",
    "Normalality of Residuals\n",
    "    \n",
    "    the error terms are going to be normally ditributed around the regression line\n",
    "    with a mean of zero, the regression line\n",
    "    deterines if two data ses come from populations with a common distribution\n",
    "    plot of the quantities of the first data set against the quantiles of the second data set\n",
    "    Q-Q plots should align on a 45degree reference line\n",
    ">>>statsmodels.graphics.gofplots.qqplot(residuals, dist=norm, line='45', fit=True)\n",
    "\n",
    "Homoscedasticity of residuals\n",
    "\n",
    "    same variance of the residuals across x\n",
    "    where the error is normally distributed with a constance variance, sigma^2\n",
    "    ie. the error term is the same across all values of the independent variables\n",
    "    if it is heterscedastic, than OLS will not hold as well and it needs a more complex model\n",
    "    Troubleshooting\n",
    "        solution might be to take log() or sqrt() of y. it can become more homoscedastic\n",
    "    Auto-correlation, if there is a pattern in the residual terms as x\n",
    "    progresses, it is a correlation where the errors in greater value x\n",
    "    are related to errors in smaller values x. this violates\n",
    "    homoscedasticity\n",
    "    \n",
    "\n",
    "No multicollinearity\n",
    "\n",
    "    multicollinearity occurs when two or more of the variables are highly correlated with each\n",
    "    other\n",
    "    ie, xj = 2xi\n",
    "    the model accuracy may look oc, but coefficents are not easily interpreted\n",
    "     X has full colunm rank\n",
    "    Troublshooting\n",
    "        correlation matrix/scatterplot matrix\n",
    "        Calculate the Variance Inflation Factor\n",
    "            run OLs linear regreesion for each predictor as a function of k-1 remaining\n",
    "            predictors\n",
    "            VIF = 1 / (1-R^2)\n",
    "                if VIF>10, problem\n",
    "                \n",
    ">>>from statsmodels.stats.outliers_influence import variance_inflation_factorvariance_inflation_factor(x.values, index)\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "What are some metrics for linear regression goodness of fit, and what do they mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "    * R**2 adjusted \n",
    "        There's adjusted R^2 that penalizes you for more dimensions that you have in your feature\n",
    "    vector. Gives you a more accurate, weighted R^2 statistic\n",
    "        (RSS/(n-p-1))/(TSS/(n-1))\n",
    "        The goal is to have a general solution so that it's applicable to new data\n",
    "    \n",
    "     *F-statistic\n",
    "        if the value is over 5 we can reject the null \n",
    "     *Mean squared error\n",
    "     * P-values\n",
    "     *Confidence interval\n",
    "         cannot straddle zero, because then we are saying one of our betas could be zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "In the context of machine learning, what are bias and variance?  And what is the bias-variance trade-off?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "our models expected squared prediction error will depend on\n",
    "\n",
    "    *how much our predictions vary\n",
    "    *the difference between the true target and our models average prediction over all possible training set\n",
    "    *the variance of the irreducible error\n",
    "\n",
    "##### Bias\n",
    "    The amount the expected value of the results differ from the true value.\n",
    "\n",
    "##### Variance\n",
    "    the expected value of the squared deviation of the results from the mean of the results.\n",
    "\n",
    "\n",
    "The ideal is low bias and low variance\n",
    "A low bias model accurately predicts the population's underlying true value or signal.\n",
    "A low variance models doesn't change much when it is fit on different data from the underlying population.\n",
    "A trade-off exists because the lower your model complexity the higher you bias. the higher your model complexity the lower your bias but the higher your variance. So as bias decreases, variance often increases. you want your model to be able to adapt and be varaible between training data\n",
    "Why are high bias models considered underfit?\n",
    "    not accurately showing the signal in your model. answers differ greatly from true value\n",
    "\n",
    "Why are high variance models considered overfit?\n",
    "    doesn't properly predict the underlying signal because it's factoring in to much noise. if you fit something off of noise, inherently variable, then it is variable. there's a more complex model, which makes it more flexible. as a result out predictions will more easily vary depending on what data we trained on. getting very different answers. Not consistent with prediction when presented with new data\n",
    "\n",
    "##### Cross Validation\n",
    "\n",
    "How can we find the best tradeoff point? (The optimum model complexity)\n",
    "\n",
    "Tune parameters, inluding model complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "Explain what kFold cross-validation is and why it's used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "### Types of cross-validation\n",
    "#### Leave one out cross-validation\n",
    "    assume we have n training samples. \n",
    "    pick one at a time to leave out, and do that for every sample in the data set\n",
    "    useful on tiny data sets\n",
    "    requires a lot of training, so it could take a long time\n",
    "\n",
    "#### k-Fold Cross-Validation\n",
    "    split the data set int k 'folds'\n",
    "    train using (k-1) folds. validated on the poriton we left out. record metrics\n",
    "    do this k times, leaving out a different fold for each one\n",
    "    general case of 'leave one out'\n",
    "    training on the large set, validating on the smaller one\n",
    "    generally k of 10 or 5.\n",
    "    remember to randomize!\n",
    "    Use k-folds to tune hyperparameters and then test against"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "What is the curse of dimensionality?  How is it addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "##### The Curse of Dimensionality\n",
    "    as the number of dimensions increases, the volume that data can occupy grows exponentially\n",
    "    \n",
    "    sampling density is proportional to the number of data points(N) you have raised to the power of 1/number of dimensions(d)\n",
    "    \n",
    "    Many more data points are needed to fill space in higher dimensions to maintain the same density of the data to build model on\n",
    "    \n",
    "    A linear model will tend to overfit in high dimensions in part because the high-dimensional data is that much more sparse\n",
    "\n",
    "    we'd like to restrict(or 'regularize') our model to reduce its variance and complexity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "We talked about L1 and L2 regularization.  What are they and in what situations might one be used instead of the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "#### Ridge Regression\n",
    "    hyperparamter-lambda\n",
    "    L equals 2\n",
    "    We're adding the square of all of the coefficients multiplied by lambda\n",
    "    when your betas are large, the cost function increases, keep betas small\n",
    "    Why would limiting the magnitude of our coefficients prevent overfitting?\n",
    "        a smaller coeffiecient has less variation in the answer\n",
    "    as model complexity increases, dimensionality increases, the coefficients start growing\n",
    "        Ridge regression penalizes that\n",
    "    not penalizing beta not\n",
    "    changing lambda changes the amount that large coefficients are penalized\n",
    "    increasing lamnda increases the model's bias and decreases its variance\n",
    "    limit the magnitude of the coefficients\n",
    "    Important to standardize your data(subtract mean, divid features by std.dev)\n",
    "        (don't want to penalize coefficients for being naturally big. this puts them on the same scale)\n",
    "        \n",
    "\n",
    "#### Lasso Regression\n",
    "    L 1 regularization\n",
    "    L equals 1\n",
    "    Taking the sum of all of the absolute values, multiplied by lambda\n",
    "        selects them out\n",
    "        the most significant features stay relevant\n",
    "        the more a coefficient tries to grow it puts it back to zero\n",
    "        starts to turn them off one by one\n",
    "    lasso tend to set coefficients exactly to zero\n",
    "        this is sort of 'automatic feature selection'\n",
    "        leads to 'sparse' models\n",
    "        serves a similar purpose to stepwise features selection\n",
    "\n",
    "#### Which one is better?\n",
    "\n",
    "    if you think all of your dimensions are meaningful--ridge\n",
    "    \n",
    "    if you think some may be redundant and that your sample spaceis small - lasso\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "Draw a confusion matrix for binary predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "\n",
    "                               predicted false    predicted true\n",
    "      negative class          True Negatives     False Positives\n",
    "  \n",
    "      positive class          False Negative     True Positives\n",
    "      \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "Give an example for each case:\n",
    "   There are many ways to evaluate the confusion matrix: \n",
    "    \n",
    "        * Accuracy: overall proportion correct (TN + TP)/(FP +FN +TN +TP)\n",
    "        * Precision: proportion called true that are correct TP/(TP + FP)\n",
    "        * Recall: proportion of true that are called correctly TP /(TP + FN)\n",
    "        * F1-Score: balancing Precision/Recall 2/((1/recall)+(1/precision))\n",
    "        \n",
    "  * Precision is more important than recall.  \n",
    "        Initial disease screenings, if you get a false positive you can go back and test\n",
    "        \n",
    "  * Recall is more important than precision.  \n",
    "        In court, you should presume guilty. So better to get a false negative\n",
    "  * You consider both to be important (and what metric would you use for that?)\n",
    "         Accurracy\n",
    "         F1 - statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    " How is a ROC curve generated?  What does it show?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "TPR plotted versus the FPR taken across thresholds. \n",
    "ROC shows how your true positve and false positive rates change in relation to each other as you increase or decrease your threshold. \n",
    " *receiver operating characteristic is a way of exploring the performance of a model by varying the decision threshold of a classifier.\n",
    "    * Area under the ROC curve(Index of accuracy, concordance index), How good is my model average over all of those thresholds?\n",
    "\n",
    "Ideal is to have the largest area\n",
    "\n",
    "<br/>\n",
    "<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "What are the similarities and differences between linear and logistic regression and how do you interpret the coefficients in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer\n",
    "* In logistic regression we are trying to model the probabilities of the K classes via linear functions in x\n",
    "    * These models are usually fit by MLE(maximum likelihood estimator)\n",
    "    * rather than model the response directlty(like in linear regression) logistic regression models the probability that Y belongs to a category\n",
    "    * e.g P(asthma | years smoked) is between 0 and 1 for any years smoked\n",
    "\n",
    "*How do we interpret β1 in a linear regression setting?\n",
    "        * holding all the other features constant, β1 gives the average change in Y associated with a one-unit increase in X\n",
    "        * There is meaning in how much each feature is weighted to your target space\n",
    " * How do we interpret β1 in a logistic regression setting? \n",
    "        * Increasing X by one unit changes the log odds by β1\n",
    "        * log odds = ln(p/1-p)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. SQL: Given table `houses` below, write a query to...\n",
    "\n",
    "| id | sqft | beds | neighborhood | type | sale_price |\n",
    "|:----------:|:------------:|:----------:|:----------:|:-----------:|:-----------:|\n",
    "| 1 | 1150 | 2 | prospect-park | townhome | 244052 |\n",
    "| 2 | 2600 | 3 | calhoun-isles | single_family | 609536 |\n",
    "| 3 | 860 | 1 | uptown | condo | 472993 |\n",
    "| 4 | 1320 | 3 | north-loop | townhome | 309485 |\n",
    "| 5 | 1030 | 2 | downtown | townhome | 456141 |\n",
    "| 6 | 3000 | 3 | uptown | single_family | 544431 |\n",
    "| 7 | 1400 | 2 | longfellow | condo | 305314 |\n",
    "| 8 | 3000 | 4 | longfellow | single_family | 485802 |\n",
    "| 9 | 1700 | 3 | stephens-square | single_family | 337029 |\n",
    "\n",
    "  * Return the average number of bedrooms and square footage for each type of home.\n",
    "    SELECT type, AVG(beds), AVG(sqft)\n",
    "    FROM houses\n",
    "    GROUP BY type\n",
    "  * Return the average sale price for each  neighborhood and home type.\n",
    "    SELECT type, neighborhood, AVG(sale_price)\n",
    "    FROM houses\n",
    "    GROUPBY type, neighborhood\n",
    "   \n",
    "   RETURN'''\n",
    "   SELECT\n",
    "   '''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
